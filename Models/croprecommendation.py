# -*- coding: utf-8 -*-
"""CropRecommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vJLWLQzCLiL4rtZQcrNpXgv3CdVBDBmd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn import tree
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Project/Datasets/Crop_recommendation.csv')
# df = pd.read_csv('Crop_recommendation.csv')

df.head(10)

df.tail()

df.info()

df.shape

df.describe()

df['label'].unique()

df.columns

df['label'].value_counts()

df.dtypes

s = df.corr()
print(s)

sns.heatmap(s,annot = True)

features = df[['N','P','K','temperature','humidity','ph','rainfall']]
target = df['label']

# Initialzing empty lists to append all model's name and corresponding name
acc = []
model = []

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(features,target,test_size= 0.2,random_state = 2)

"""**DECISION TREE**"""

from sklearn.tree import DecisionTreeClassifier

DecisionTree = DecisionTreeClassifier(criterion = 'entropy',max_depth = 5,random_state = 2)
DecisionTree.fit(X_train,y_train)
predicted = DecisionTree.predict(X_test)
x = metrics.accuracy_score(y_test,predicted)
acc.append(x)
model.append('Decision Tree')
print("Decision Tree's accuracy is", x * 100)

print(classification_report(y_test,predicted))

score = cross_val_score(DecisionTree,features,target,cv = 5)

score

"""**NAIVE BAYES(GAUSSIAN)**"""

from sklearn.naive_bayes import GaussianNB

Naive_Bayes = GaussianNB()
Naive_Bayes.fit(X_train,y_train)

predicted = Naive_Bayes.predict(X_test)
X = metrics.accuracy_score(y_test,predicted)
acc.append(x)
model.append('Naive Bayes')
print('Naive Bayes accuracy is',x * 100)

print(classification_report(y_test,predicted))

score = cross_val_score(Naive_Bayes,features,target,cv = 5)

score

"""**LOGISTIC REGRESSION**"""

from sklearn.linear_model import LogisticRegression

LogReg = LogisticRegression()
LogReg.fit(X_train,y_train)

predicted = LogReg.predict(X_test)
x = metrics.accuracy_score(y_test,predicted)
acc.append(x)
model.append('Logistic Regression')
print("Logistic Regression Accuracy is",x * 100)
print(classification_report(y_test,predicted))

score = cross_val_score(LogReg,features,target,cv = 5)

score

"""**K-Nearest Neighbors(KNN)**"""

from sklearn.neighbors import KNeighborsClassifier

KNN = KNeighborsClassifier()
KNN.fit(X_train,y_train)
predicted = KNN.predict(X_test)
x = metrics.accuracy_score(y_test, predicted)
acc.append(x)
model.append('K Nearest Neighbours')
print("KNN Accuracy is: ", x * 100)
print(classification_report(y_test,predicted))

score = cross_val_score(KNN,features,target,cv = 5)

score

"""**RANDOM FOREST**"""

from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier(n_estimators=29, criterion = 'entropy', random_state=0)
RF.fit(X_train,y_train)
predicted = RF.predict(X_test)
x = metrics.accuracy_score(y_test,predicted)
acc.append(x)
model.append('Random Forest')
print("Random Forest Accuracy is ",x * 100)
print(classification_report(y_test,predicted))

score = cross_val_score(RF,features,target,cv = 5)

score

"""**XG BOOST**"""

#import xgboost as xgb

"""XB = xgb.XGBClassifier(eval_metric='mlogloss')
XB.fit(X_train,y_train)
predicted = XB.predict(X_test)
x = metrics.accuracy_score(y_test,predicted)
acc.append(x)
model.append('XG Boost')
print('XG Boost Accuracy is ',x * 100)
print(classification_report(y_test,predicted))"""

#score = cross_val_score(XB,features,target,cv = 5)

#score

"""**ACCURACY COMPARISION**"""

plt.figure(figsize = [12,8],dpi = 100)
plt.title('Accuracy Comparision')
plt.xlabel('Accuracy')
plt.ylabel('Algorithms')
sns.barplot(x = acc,y = model,palette = 'dark')

accuracy_models = dict(zip(model,acc))
for k,v in accuracy_models.items():
  print(k,'-->',v* 100,'%')

"""**PREDICTION**"""

data = np.array([[90, 42, 43, 20,82, 6.50, 202.9]])
prediction = RF.predict(data)
print(prediction)

import pickle
#saving the model by using pickle function
pickle.dump(RF, open('FR.pkl','wb'))

"""**CONCLUSION**

We are using Random Forest because of its accuracy and precision.
"""